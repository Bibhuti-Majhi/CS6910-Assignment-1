{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist, fashion_mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import wandb\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 16\n",
    "no_of_features = 784\n",
    "no_of_classes = 10\n",
    "no_of_layers = 5\n",
    "no_of_neurons_in_each_layer = [128,128,128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X,Y),(X_test,Y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_label = []\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "fig,axes = plt.subplots(2,5,figsize=(10,10))\n",
    "axes = axes.flatten()\n",
    "index = 0\n",
    "for x in range(len(X_test)):\n",
    "    if(Y_test[x] not in visited_label):\n",
    "        visited_label.append(Y_test[x])\n",
    "        axes[index].imshow(X_test[x],cmap = plt.cm.gray)\n",
    "        axes[index].set_title(\"{}\".format(class_names[Y_test[x]]))\n",
    "        index += 1\n",
    "plt.show()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(len(X),784,1)\n",
    "X[0].shape\n",
    "X_test = X_test.reshape(len(X_test),784,1)\n",
    "X_test[0].shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X/255.0\n",
    "X_test = X_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,Y_train,Y_val = train_test_split(X,Y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(l,no_of_classes):\n",
    "    temp = np.array([0]*no_of_classes)\n",
    "    temp[l] = 1\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(Y_hat,Y):\n",
    "    temp = []\n",
    "    for x in range(len(Y)):\n",
    "        temp.append(one_hot(Y[x],no_of_classes))\n",
    "    temp = np.array(temp)\n",
    "    return (-1.0 * np.sum(np.multiply(temp,np.log(Y_hat+1e-9).reshape(Y_hat.shape[0],Y_hat.shape[1]))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1.+np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (np.ones_like(x)-sigmoid(x))\n",
    "\n",
    "def Relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def Relu_derivative(x):\n",
    "    return 1*(x>0) \n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 - (np.tanh(x)**2))\n",
    "\n",
    "def softmax(x):\n",
    "    l = []\n",
    "    for i in range(len(x)):\n",
    "        l.append(np.exp(x[i])/np.sum(np.exp(x[i]),axis=0))\n",
    "    return np.array(l)\n",
    "\n",
    "\n",
    "def softmax_derivative(x):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = x[i] / max(x[i])\n",
    "    return softmax(x) * (1-softmax(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass layers as no of layers including hidden,input and output\n",
    "#pass no_of_neurons as no of neurons in each hidden layer\n",
    "def feed_forward(data,thetas,layers):\n",
    "    pre_activation = [1]*(layers)\n",
    "    activation  = [1]*(layers)\n",
    "    activation[0] = data\n",
    "    for layer_no in range(1,layers):\n",
    "        W = 'W' + str(layer_no)\n",
    "        b = 'b' + str(layer_no)\n",
    "        pre_activation[layer_no] = np.add(np.matmul(thetas[W],activation[layer_no - 1]),thetas[b])\n",
    "        if(layer_no == layers-1):\n",
    "            activation[layer_no] = softmax(pre_activation[layer_no])\n",
    "        else:\n",
    "            activation[layer_no] = sigmoid(pre_activation[layer_no])\n",
    "    return activation,pre_activation\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(no_of_features,no_of_classes,no_of_layers,no_of_neurons_in_each_layer):\n",
    "    np.random.seed(42)\n",
    "    thetas = {}\n",
    "    for layer in range(1,no_of_layers):\n",
    "        if(layer == 1):\n",
    "            thetas['W'+str(layer)] = np.random.randn(no_of_neurons_in_each_layer[layer-1],no_of_features) #* np.sqrt(2/(no_of_neurons_in_each_layer[layer-1]+no_of_features))\n",
    "            thetas['b'+str(layer)] = np.zeros((no_of_neurons_in_each_layer[layer-1],1))\n",
    "        elif(layer == no_of_layers-1):\n",
    "            thetas['W'+str(layer)] = np.random.randn(no_of_classes,no_of_neurons_in_each_layer[layer-2]) #* np.sqrt(2/(no_of_classes + no_of_neurons_in_each_layer[layer-2]))\n",
    "            thetas['b'+str(layer)] = np.zeros((no_of_classes,1))\n",
    "        else:\n",
    "            thetas['W'+str(layer)] = np.random.randn(no_of_neurons_in_each_layer[layer-1],no_of_neurons_in_each_layer[layer-2]) #* np.sqrt(2/(no_of_neurons_in_each_layer[layer-1]+no_of_neurons_in_each_layer[layer-2]))\n",
    "            thetas['b'+str(layer)] = np.zeros((no_of_neurons_in_each_layer[layer-1],1))\n",
    "    return thetas\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(no_of_features,no_of_classes,no_of_layers,no_of_neurons_in_each_layer):\n",
    "    thetas = {}\n",
    "    np.random.seed(42)\n",
    "    for layer in range(1,no_of_layers):\n",
    "        if(layer == 1):\n",
    "            thetas['W'+str(layer)] = np.random.default_rng().uniform(low = -0.7,high =0.7,size = (no_of_neurons_in_each_layer[layer-1],no_of_features)) #* np.sqrt(2/(no_of_neurons_in_each_layer[layer-1]+no_of_features))\n",
    "            thetas['b'+str(layer)] = np.random.default_rng().uniform(low = -0.7,high =0.7,size = (no_of_neurons_in_each_layer[layer-1],1))\n",
    "        elif(layer == no_of_layers-1):\n",
    "            thetas['W'+str(layer)] = np.random.default_rng().uniform(low = -0.7,high =0.7,size = (no_of_classes,no_of_neurons_in_each_layer[layer-2])) #* np.sqrt(2/(no_of_classes + no_of_neurons_in_each_layer[layer-2]))\n",
    "            thetas['b'+str(layer)] = np.random.default_rng().uniform(low = -0.7,high =0.7,size = (no_of_classes,1))\n",
    "        else:\n",
    "            thetas['W'+str(layer)] = np.random.default_rng().uniform(low = -0.7,high =0.7,size =(no_of_neurons_in_each_layer[layer-1],no_of_neurons_in_each_layer[layer-2])) #*  np.sqrt(2/(no_of_neurons_in_each_layer[layer-1]+no_of_neurons_in_each_layer[layer-2]))\n",
    "            thetas['b'+str(layer)] = np.random.default_rng().uniform(low = -0.7,high =0.7,size = (no_of_neurons_in_each_layer[layer-1],1))\n",
    "    return thetas\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(no_of_features,no_of_classes,no_of_layers,no_of_neurons_in_each_layer):\n",
    "    thetas = {}\n",
    "    np.random.seed(42)\n",
    "    for layer in range(1,no_of_layers):\n",
    "        if(layer == 1):\n",
    "            thetas['W'+str(layer)] = np.random.uniform(low = -0.7,high =0.7,size = (no_of_neurons_in_each_layer[layer-1],no_of_features)) #* np.sqrt(2/(no_of_neurons_in_each_layer[layer-1]+no_of_features))\n",
    "            thetas['b'+str(layer)] = np.zeros((no_of_neurons_in_each_layer[layer-1],1))\n",
    "        elif(layer == no_of_layers-1):\n",
    "            thetas['W'+str(layer)] = np.random.uniform(low = -0.7,high =0.7,size = (no_of_classes,no_of_neurons_in_each_layer[layer-2])) #* np.sqrt(2/(no_of_classes + no_of_neurons_in_each_layer[layer-2]))\n",
    "            thetas['b'+str(layer)] = np.zeros((no_of_classes,1))\n",
    "        else:\n",
    "            thetas['W'+str(layer)] = np.random.uniform(low = -0.7,high =0.7,size =(no_of_neurons_in_each_layer[layer-1],no_of_neurons_in_each_layer[layer-2])) #*  np.sqrt(2/(no_of_neurons_in_each_layer[layer-1]+no_of_neurons_in_each_layer[layer-2]))\n",
    "            thetas['b'+str(layer)] = np.zeros((no_of_neurons_in_each_layer[layer-1],1))\n",
    "    return thetas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grad wrt output layer preactivation\n",
    "# correct\n",
    "def compute_grad_preactivation_output(activation,Y):\n",
    "    grads = []\n",
    "    for x in range(len(activation[-1])):\n",
    "        act = activation[-1][x]\n",
    "        grad = np.array([0]*len(act)).reshape(len(act),1)\n",
    "        index = Y[x]\n",
    "        grad[index] = 1\n",
    "        grads.append(-(grad - act))\n",
    "    return np.array(grads)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad_weight(grad_ak,hk_1):\n",
    "    temp = []\n",
    "    for x in range(len(grad_ak)):\n",
    "        temp.append(np.matmul(grad_ak[x],hk_1[x].T))\n",
    "    return np.array(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad_activation(wk,grad_ak):\n",
    "    return np.matmul(wk.T,grad_ak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad_preactivation(grad_hk_1,ak_1):\n",
    "    return np.multiply(grad_hk_1,sigmoid_derivative(ak_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagate(activation,preactivation,thetas,Y):\n",
    "    grads = {}\n",
    "    grads['a' + str(no_of_layers-1)] = compute_grad_preactivation_output(activation,Y)\n",
    "    for k in range(no_of_layers-1,0,-1):\n",
    "        grads['W'+str(k)] = np.sum(compute_grad_weight(grads['a' + str(k)],activation[k-1]),axis = 0)/batchsize\n",
    "        grads['b'+str(k)] = np.sum(grads['a' + str(k)],axis = 0)/batchsize\n",
    "        if(k == 1):\n",
    "            break\n",
    "        grads['h'+str(k-1)] = compute_grad_activation(thetas['W'+str(k)],grads['a'+str(k)])\n",
    "        grads['a'+str(k-1)] = compute_grad_preactivation(grads['h'+str(k-1)],preactivation[k-1])\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(optimizer):\n",
    "    thetas = initialization(no_of_features,no_of_classes,no_of_layers,no_of_neurons_in_each_layer)\n",
    "    max_epochs = 15\n",
    "    eta = 0.001\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.99\n",
    "    delta = 1e-5\n",
    "    grads = {}\n",
    "    for i in thetas.keys():\n",
    "        grads[i] = 0\n",
    "    for t in range(max_epochs):\n",
    "        #previous_update\n",
    "        ut = {}\n",
    "        vt = {}\n",
    "        for i in thetas.keys():\n",
    "            ut[i] = 0\n",
    "            vt[i] = 0\n",
    "        params_look_ahead = {}\n",
    "        for x in range(0,X_train.shape[0],batchsize):\n",
    "            if(optimizer == 'nesterov'):\n",
    "                for i in thetas.keys():\n",
    "                    params_look_ahead[i] = thetas[i] - beta*ut[i]\n",
    "                activation,preactivation = feed_forward(X_train[x:x+batchsize],thetas,no_of_layers)\n",
    "                grads = back_propagate(activation,preactivation,params_look_ahead,Y_train[x:x+batchsize])\n",
    "                for i in thetas.keys():\n",
    "                    ut[i] = beta1*ut[i] + (1-beta1)*grads[i]\n",
    "                    thetas[i] = thetas[i] - eta*ut[i]\n",
    "            elif(optimizer == 'mgd'):\n",
    "                activation,preactivation = feed_forward(X_train[x:x+batchsize],thetas,no_of_layers)\n",
    "                grads = back_propagate(activation,preactivation,thetas,Y_train[x:x+batchsize])     \n",
    "                for i in thetas.keys():\n",
    "                    ut[i] = beta1*ut[i] + grads[i]\n",
    "                    thetas[i] = thetas[i] - eta*ut[i]\n",
    "            elif(optimizer == 'sgd'):\n",
    "                activation,preactivation = feed_forward(X_train[x:x+batchsize],thetas,no_of_layers)\n",
    "                grads = back_propagate(activation,preactivation,thetas,Y_train[x:x+batchsize])\n",
    "                for i in thetas.keys():\n",
    "                    thetas[i] = thetas[i] - eta*grads[i]\n",
    "            elif(optimizer == 'RMSprop'):\n",
    "                activation,preactivation = feed_forward(X_train[x:x+batchsize],thetas,no_of_layers)\n",
    "                grads = back_propagate(activation,preactivation,thetas,Y_train[x:x+batchsize])\n",
    "                for i in thetas.keys():\n",
    "                    ut[i] = beta1*ut[i] + (1-beta1)*np.multiply(grads[i],grads[i])\n",
    "                    thetas[i] = thetas[i] - eta*grads[i]/((np.sqrt(ut[i])+delta))\n",
    "            elif(optimizer == 'adam'):\n",
    "                activation,preactivation = feed_forward(X_train[x:x+batchsize],thetas,no_of_layers)\n",
    "                grads = back_propagate(activation,preactivation,thetas,Y_train[x:x+batchsize])\n",
    "                for i in thetas.keys():\n",
    "                    ut[i] = beta1*ut[i] + (1-beta1)*grads[i]\n",
    "                    uthat = ut[i]/(1 - pow(beta1,t+1))\n",
    "                    vt[i] = beta2*vt[i] + (1-beta2)*np.multiply(grads[i],grads[i])\n",
    "                    vthat = vt[i]/(1 - pow(beta2,t+1))\n",
    "                    thetas[i] = thetas[i] - eta*uthat/((np.sqrt(vthat) + delta))\n",
    "            elif(optimizer == 'nadam'):\n",
    "                activation,preactivation = feed_forward(X_train[x:x+batchsize],thetas,no_of_layers)\n",
    "                grads = back_propagate(activation,preactivation,thetas,Y_train[x:x+batchsize])\n",
    "                for i in thetas.keys():\n",
    "                    ut[i] = beta1*ut[i] + (1-beta1)*grads[i]\n",
    "                    uthat = ut[i]/(1 - pow(beta1,t+1))\n",
    "                    vt[i] = beta2*vt[i] + (1-beta2)*np.multiply(grads[i],grads[i])\n",
    "                    vthat = vt[i]/(1 - pow(beta2,t+1))\n",
    "                    thetas[i] = thetas[i] - (eta*(beta1*uthat + (1-beta1)*grads[i]/(1-pow(beta1,t+1))))/(np.sqrt(vthat) + delta)\n",
    "        ac,pre = feed_forward(X_train[:],thetas,no_of_layers)\n",
    "        print_accuracy(ac,Y_train)   \n",
    "            \n",
    "    return thetas\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 32\n",
    "no_of_features = 784\n",
    "no_of_classes = 10\n",
    "no_of_layers = 5\n",
    "no_of_neurons_in_each_layer = [128,128,128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_accuracy(yhat,y):\n",
    "    correct = 0\n",
    "    for x in range(len(yhat[-1])):\n",
    "        if(np.argmax(yhat[-1][x]) == y[x]):\n",
    "            correct+=1\n",
    "    print(correct/len(y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.46458333333334\n",
      "87.73333333333333\n",
      "88.42291666666667\n",
      "89.12916666666668\n",
      "89.60625\n",
      "90.10208333333334\n",
      "90.5\n",
      "90.84791666666666\n",
      "91.19583333333333\n",
      "91.52083333333333\n",
      "91.80833333333334\n",
      "92.06041666666667\n",
      "92.24375\n",
      "92.48333333333333\n",
      "92.72500000000001\n"
     ]
    }
   ],
   "source": [
    "t = gradient_descent(optimizer='nadam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,h = feed_forward(X_test[:],t,no_of_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.03\n"
     ]
    }
   ],
   "source": [
    "print_accuracy(a,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,h = feed_forward(X_train[:],t,no_of_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.72500000000001\n"
     ]
    }
   ],
   "source": [
    "print_accuracy(a,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
