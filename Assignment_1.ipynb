{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist, fashion_mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import wandb\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 16\n",
    "no_of_features = 784\n",
    "no_of_classes = 10\n",
    "no_of_layers = 5\n",
    "no_of_neurons_in_each_layer = [128,128,128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X,Y),(X_test,Y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_label = []\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "fig,axes = plt.subplots(2,5,figsize=(10,10))\n",
    "axes = axes.flatten()\n",
    "index = 0\n",
    "for x in range(len(X_test)):\n",
    "    if(Y_test[x] not in visited_label):\n",
    "        visited_label.append(Y_test[x])\n",
    "        axes[index].imshow(X_test[x],cmap = plt.cm.gray)\n",
    "        axes[index].set_title(\"{}\".format(class_names[Y_test[x]]))\n",
    "        index += 1\n",
    "plt.show()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(len(X),784,1)\n",
    "X[0].shape\n",
    "X_test = X_test.reshape(len(X_test),784,1)\n",
    "X_test[0].shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitStandarization(x):\n",
    "        mean=np.mean(x,axis=0)\n",
    "        var=np.var(x,axis=0)\n",
    "        sd=np.sqrt(var)\n",
    "        for i in range(x.shape[1]):\n",
    "            x[:,i]= x[:,i]-mean[i]\n",
    "            x[:,i]=x[:,i]/sd[i]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X/255.0\n",
    "X_test = X_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,Y_train,Y_val = train_test_split(X,Y,test_size=0.1,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = fitStandarization(X_train)\n",
    "X_test = fitStandarization(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(Y_hat,Y):\n",
    "    temp = []\n",
    "    for x in range(len(Y)):\n",
    "        temp.append(one_hot(Y[x],no_of_classes))\n",
    "    temp = np.array(temp)\n",
    "    return (-1.0 * np.sum(np.multiply(temp,np.log(Y_hat+1e-9).reshape(Y_hat.shape[0],Y_hat.shape[1]))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grad wrt output layer preactivation\n",
    "# correct\n",
    "def compute_grad_preactivation_output(activation,Y):\n",
    "    grads = []\n",
    "    for x in range(len(activation[-1])):\n",
    "        act = activation[-1][x]\n",
    "        grad = np.array([0]*len(act)).reshape(len(act),1)\n",
    "        index = Y[x]\n",
    "        grad[index] = 1\n",
    "        grads.append(-(grad - act))\n",
    "    return np.array(grads)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad_weight(grad_ak,hk_1):\n",
    "    temp = []\n",
    "    for x in range(len(grad_ak)):\n",
    "        temp.append(np.matmul(grad_ak[x],hk_1[x].T))\n",
    "    return np.array(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad_activation(wk,grad_ak):\n",
    "    return np.matmul(wk.T,grad_ak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad_preactivation(grad_hk_1,ak_1):\n",
    "    return np.multiply(grad_hk_1,sigmoid_derivative(ak_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagate(activation,preactivation,thetas,Y):\n",
    "    grads = {}\n",
    "    grads['a' + str(no_of_layers-1)] = compute_grad_preactivation_output(activation,Y)\n",
    "    for k in range(no_of_layers-1,0,-1):\n",
    "        grads['W'+str(k)] = np.sum(compute_grad_weight(grads['a' + str(k)],activation[k-1]),axis = 0)/batchsize\n",
    "        grads['b'+str(k)] = np.sum(grads['a' + str(k)],axis = 0)/batchsize\n",
    "        if(k == 1):\n",
    "            break\n",
    "        grads['h'+str(k-1)] = compute_grad_activation(thetas['W'+str(k)],grads['a'+str(k)])\n",
    "        grads['a'+str(k-1)] = compute_grad_preactivation(grads['h'+str(k-1)],preactivation[k-1])\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagate(h,a,thetas,Y):\n",
    "    grads = {}\n",
    "    for x in thetas.keys():\n",
    "        grads[x] = 0\n",
    "    for x in range(len(Y)):\n",
    "        temp = h[-1][x] - one_hot(Y[x],no_of_classes).reshape(no_of_classes,1)\n",
    "        for k in range(no_of_layers-1,0,-1):\n",
    "            W = 'W' + str(k)\n",
    "            b = 'b' + str(k)\n",
    "            grads[W] += np.matmul(temp,h[k-1][x].T)\n",
    "            grads[b] += temp\n",
    "            if(k == 1):\n",
    "                break\n",
    "            temp = np.matmul(thetas[W].T,temp)\n",
    "            temp = np.multiply(temp,Relu_derivative(a[k-1][x]))\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self,optimizer,batchsize,no_of_features,no_of_classes,no_of_layers,no_of_neurons_in_each_layer,max_epochs,eta,initialization_method,activation_method,loss):\n",
    "        self.optimizer  = optimizer\n",
    "        self.batchsize = batchsize\n",
    "        self.no_of_features = no_of_features\n",
    "        self.no_of_classes = no_of_classes\n",
    "        self.no_of_layers = no_of_layers\n",
    "        self.no_of_neurons_in_each_layer = no_of_neurons_in_each_layer\n",
    "        self.max_epochs = max_epochs\n",
    "        self.eta = eta\n",
    "        self.initialization = initialization_method\n",
    "        self.initialization_list = {'xavier':self.xavier_initialization,'uniform':self.uniform_initialization,'normal':self.normal_initialization,'he':self.he_initialization}\n",
    "        self.activation = activation_method\n",
    "        self.activation_list = {'sigmoid':self.sigmoid,'relu':self.Relu,'tanh':self.tanh}\n",
    "        self.activation_derivative = {'sigmoid':self.sigmoid_derivative,'relu':self.Relu_derivative,'tanh':self.tanh_derivative}\n",
    "        self.loss = loss\n",
    "        self.thetas = {}\n",
    "        self.loss_list = []\n",
    "    def one_hot(self,l,no_of_classes):\n",
    "        temp = np.array([0]*no_of_classes)\n",
    "        temp[l] = 1\n",
    "        return temp\n",
    "    def sigmoid(self,x):\n",
    "        return 1. / (1.+np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self,x):\n",
    "        return self.sigmoid(x) * (np.ones_like(x)-self.sigmoid(x))\n",
    "\n",
    "    def Relu(self,x):\n",
    "        # for i in range(len(x)):\n",
    "        #     x[i] = x[i] / max(x[i])\n",
    "        return np.maximum(0,x)\n",
    "\n",
    "    def Relu_derivative(self,x):\n",
    "        # for i in range(len(x)):\n",
    "        #     x[i] = x[i] / max(x[i])\n",
    "        return 1*(x>0) \n",
    "\n",
    "    def tanh(self,x):\n",
    "        # for i in range(len(x)):\n",
    "        #     x[i] = x[i] / max(x[i])\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def tanh_derivative(self,x):\n",
    "        # for i in range(len(x)):\n",
    "        #     x[i] = x[i] / max(x[i])\n",
    "        return (1 - (np.tanh(x)**2))\n",
    "\n",
    "    def softmax(self,x):\n",
    "        # for i in range(len(x)):\n",
    "        #     x[i] = x[i] / max(x[i])\n",
    "        l = []\n",
    "        for i in range(len(x)):\n",
    "            l.append(np.exp(x[i])/np.sum(np.exp(x[i]),axis=0))\n",
    "        return np.array(l)\n",
    "        \n",
    "    def softmax_derivative(self,x):\n",
    "        for i in range(len(x)):\n",
    "            x[i] = x[i] / max(x[i])\n",
    "        return self.softmax(x) * (1-self.softmax(x))\n",
    "\n",
    "    def he_initialization(self):\n",
    "        np.random.seed(42)\n",
    "        thetas = {}\n",
    "        for layer in range(1,self.no_of_layers):\n",
    "            if(layer == 1):\n",
    "                thetas['W'+str(layer)] = np.random.normal(0,1,size = (self.no_of_neurons_in_each_layer[layer-1],self.no_of_features)) * np.sqrt(2/(self.no_of_neurons_in_each_layer[layer-1]))\n",
    "                thetas['b'+str(layer)] = np.random.normal(0,1,size = (self.no_of_neurons_in_each_layer[layer-1],1))*np.sqrt(2/self.no_of_neurons_in_each_layer[layer-1])\n",
    "            elif(layer == self.no_of_layers-1):\n",
    "                thetas['W'+str(layer)] = np.random.normal(0,1,size = (self.no_of_classes,self.no_of_neurons_in_each_layer[layer-2])) * np.sqrt(2/(self.no_of_classes))\n",
    "                thetas['b'+str(layer)] = np.random.normal(0,1,size = (self.no_of_classes,1)) * np.sqrt(2/(self.no_of_classes))\n",
    "            else:\n",
    "                thetas['W'+str(layer)] = np.random.normal(0,1,size = (self.no_of_neurons_in_each_layer[layer-1],self.no_of_neurons_in_each_layer[layer-2])) * np.sqrt(2/(self.no_of_neurons_in_each_layer[layer-1]))\n",
    "                thetas['b'+str(layer)] = np.random.normal(0,1,size = (self.no_of_neurons_in_each_layer[layer-1],1)) * np.sqrt(2/self.no_of_neurons_in_each_layer[layer-1])\n",
    "        return thetas\n",
    "\n",
    "    def xavier_initialization(self):\n",
    "        np.random.seed(42)\n",
    "        thetas = {}\n",
    "        for layer in range(1,self.no_of_layers):\n",
    "            if(layer == 1):\n",
    "                thetas['W'+str(layer)] = np.random.randn(self.no_of_neurons_in_each_layer[layer-1],self.no_of_features) * np.sqrt(2/(self.no_of_neurons_in_each_layer[layer-1]+self.no_of_features))\n",
    "                thetas['b'+str(layer)] = np.zeros((self.no_of_neurons_in_each_layer[layer-1],1))\n",
    "            elif(layer == self.no_of_layers-1):\n",
    "                thetas['W'+str(layer)] = np.random.randn(self.no_of_classes,no_of_neurons_in_each_layer[layer-2]) * np.sqrt(2/(self.no_of_classes + self.no_of_neurons_in_each_layer[layer-2]))\n",
    "                thetas['b'+str(layer)] = np.zeros((self.no_of_classes,1))\n",
    "            else:\n",
    "                # print(layer)\n",
    "                # print(self.no_of_neurons_in_each_layer[layer-1])\n",
    "                # print(self.no_of_neurons_in_each_layer[layer-2])\n",
    "                thetas['W'+str(layer)] = np.random.randn(self.no_of_neurons_in_each_layer[layer-1],self.no_of_neurons_in_each_layer[layer-2]) * np.sqrt(2/(self.no_of_neurons_in_each_layer[layer-1]+self.no_of_neurons_in_each_layer[layer-2]))\n",
    "                thetas['b'+str(layer)] = np.zeros((self.no_of_neurons_in_each_layer[layer-1],1))\n",
    "        return thetas\n",
    "\n",
    "    def uniform_initialization(self):\n",
    "        thetas = {}\n",
    "        np.random.seed(42)\n",
    "        for layer in range(1,self.no_of_layers):\n",
    "            if(layer == 1):\n",
    "                thetas['W'+str(layer)] = np.random.default_rng().uniform(low = -0.7,high =0.7,size = (self.no_of_neurons_in_each_layer[layer-1],self.no_of_features)) #* np.sqrt(2/(no_of_neurons_in_each_layer[layer-1]+no_of_features))\n",
    "                thetas['b'+str(layer)] = np.random.default_rng().uniform(low = -0.7,high =0.7,size = (self.no_of_neurons_in_each_layer[layer-1],1))\n",
    "            elif(layer == self.no_of_layers-1):\n",
    "                thetas['W'+str(layer)] = np.random.default_rng().uniform(low = -0.7,high =0.7,size = (self.no_of_classes,self.no_of_neurons_in_each_layer[layer-2])) #* np.sqrt(2/(no_of_classes + no_of_neurons_in_each_layer[layer-2]))\n",
    "                thetas['b'+str(layer)] = np.random.default_rng().uniform(low = -0.7,high =0.7,size = (self.no_of_classes,1))\n",
    "            else:\n",
    "                thetas['W'+str(layer)] = np.random.default_rng().uniform(low = -0.7,high =0.7,size =(self.no_of_neurons_in_each_layer[layer-1],self.no_of_neurons_in_each_layer[layer-2])) #*  np.sqrt(2/(no_of_neurons_in_each_layer[layer-1]+no_of_neurons_in_each_layer[layer-2]))\n",
    "                thetas['b'+str(layer)] = np.random.default_rng().uniform(low = -0.7,high =0.7,size = (self.no_of_neurons_in_each_layer[layer-1],1))\n",
    "        return thetas\n",
    "\n",
    "    def normal_initialization(self):\n",
    "        thetas = {}\n",
    "        np.random.seed(42)\n",
    "        for layer in range(1,self.no_of_layers):\n",
    "            if(layer == 1):\n",
    "                thetas['W'+str(layer)] = np.random.uniform(low = -0.7,high =0.7,size = (self.no_of_neurons_in_each_layer[layer-1],self.no_of_features)) #* np.sqrt(2/(no_of_neurons_in_each_layer[layer-1]+no_of_features))\n",
    "                thetas['b'+str(layer)] = np.zeros((self.no_of_neurons_in_each_layer[layer-1],1))\n",
    "            elif(layer == self.no_of_layers-1):\n",
    "                thetas['W'+str(layer)] = np.random.uniform(low = -0.7,high =0.7,size = (self.no_of_classes,self.no_of_neurons_in_each_layer[layer-2])) #* np.sqrt(2/(no_of_classes + no_of_neurons_in_each_layer[layer-2]))\n",
    "                thetas['b'+str(layer)] = np.zeros((self.no_of_classes,1))\n",
    "            else:\n",
    "                thetas['W'+str(layer)] = np.random.uniform(low = -0.7,high =0.7,size =(self.no_of_neurons_in_each_layer[layer-1],self.no_of_neurons_in_each_layer[layer-2])) #*  np.sqrt(2/(no_of_neurons_in_each_layer[layer-1]+no_of_neurons_in_each_layer[layer-2]))\n",
    "                thetas['b'+str(layer)] = np.zeros((self.no_of_neurons_in_each_layer[layer-1],1))\n",
    "        return thetas\n",
    "\n",
    "    def feed_forward(self,data,thetas,layers):\n",
    "        pre_activation = [1]*(layers)\n",
    "        activation  = [1]*(layers)\n",
    "        activation[0] = data\n",
    "        for layer_no in range(1,layers):\n",
    "            W = 'W' + str(layer_no)\n",
    "            b = 'b' + str(layer_no)\n",
    "            pre_activation[layer_no] = np.add(np.matmul(thetas[W],activation[layer_no - 1]),thetas[b])\n",
    "            if(layer_no == layers-1):\n",
    "                activation[layer_no] = self.softmax(pre_activation[layer_no])\n",
    "            else:\n",
    "                activation[layer_no] = self.activation_list[self.activation](pre_activation[layer_no])\n",
    "        return activation,pre_activation\n",
    "\n",
    "    def back_propagate(self,h,a,thetas,Y):\n",
    "        grads = {}\n",
    "        for x in thetas.keys():\n",
    "            grads[x] = 0\n",
    "        for x in range(len(Y)):\n",
    "            temp = h[-1][x] - self.one_hot(Y[x],self.no_of_classes).reshape(self.no_of_classes,1)\n",
    "            for k in range(self.no_of_layers-1,0,-1):\n",
    "                W = 'W' + str(k)\n",
    "                b = 'b' + str(k)\n",
    "                grads[W] += np.matmul(temp,h[k-1][x].T)/self.batchsize\n",
    "                grads[b] += temp/self.batchsize\n",
    "                if(k == 1):\n",
    "                    break\n",
    "                temp = np.matmul(thetas[W].T,temp)\n",
    "                temp = np.multiply(temp,self.activation_derivative[self.activation](a[k-1][x]))\n",
    "        return grads\n",
    "\n",
    "    def momentumUpdate(self,t,maxm=.999):\n",
    "        x=np.log(np.floor(t/250)+1)/np.log(2)\n",
    "        x=1-2**(-1-x)\n",
    "        return min(x,maxm)\n",
    "\n",
    "    def getGamma(self,epoch):\n",
    "        x=np.log((epoch/250)+1)\n",
    "        x=-1-1*(x)\n",
    "        x=2**x\n",
    "        x=1-x\n",
    "        return min(x,.9)\n",
    "\n",
    "    def fit(self,X_train,Y_train):\n",
    "        self.thetas = self.initialization_list[self.initialization]()\n",
    "        delta = 1e-9\n",
    "        grads = {}\n",
    "        for i in self.thetas.keys():\n",
    "            grads[i] = 0\n",
    "        for t in range(self.max_epochs):\n",
    "            #previous_update\n",
    "            ut = {}\n",
    "            vt = {}\n",
    "            gamma = self.getGamma(t+1)\n",
    "            beta = self.momentumUpdate(t+1)\n",
    "            for i in self.thetas.keys():\n",
    "                ut[i] = 0\n",
    "                vt[i] = 0\n",
    "            params_look_ahead = {}\n",
    "            step = 1\n",
    "            for x in range(0,X_train.shape[0],self.batchsize):\n",
    "                beta1 = 0.9#self.momentumUpdate(step)\n",
    "                beta2 = 0.99#self.momentumUpdate(step)\n",
    "                if(self.optimizer == 'nesterov'):\n",
    "                    for i in self.thetas.keys():\n",
    "                        params_look_ahead[i] = self.thetas[i] - beta1*ut[i]\n",
    "                    activation,preactivation = self.feed_forward(X_train[x:x+self.batchsize],self.thetas,self.no_of_layers)\n",
    "                    grads = self.back_propagate(activation,preactivation,params_look_ahead,Y_train[x:x+self.batchsize])\n",
    "                    for i in self.thetas.keys():\n",
    "                        ut[i] = beta1*ut[i] + (1-beta1)*grads[i]\n",
    "                        self.thetas[i] = self.thetas[i] - self.eta*ut[i]\n",
    "                elif(self.optimizer == 'mgd'):\n",
    "                    activation,preactivation = self.feed_forward(X_train[x:x+self.batchsize],self.thetas,self.no_of_layers)\n",
    "                    grads = self.back_propagate(activation,preactivation,self.thetas,Y_train[x:x+self.batchsize])     \n",
    "                    for i in self.thetas.keys():\n",
    "                        ut[i] = gamma*ut[i] + grads[i]\n",
    "                        self.thetas[i] = self.thetas[i] - self.eta*ut[i]\n",
    "                elif(self.optimizer == 'sgd'):\n",
    "                    activation,preactivation = self.feed_forward(X_train[x:x+self.batchsize],self.thetas,self.no_of_layers)\n",
    "                    grads = self.back_propagate(activation,preactivation,self.thetas,Y_train[x:x+self.batchsize])\n",
    "                    for i in self.thetas.keys():\n",
    "                        self.thetas[i] = self.thetas[i] - self.eta*grads[i]\n",
    "                elif(self.optimizer == 'RMSprop'):\n",
    "                    activation,preactivation = self.feed_forward(X_train[x:x+self.batchsize],self.thetas,self.no_of_layers)\n",
    "                    grads = self.back_propagate(activation,preactivation,self.thetas,Y_train[x:x+self.batchsize])\n",
    "                    for i in self.thetas.keys():\n",
    "                        ut[i] = beta*ut[i] + (1-beta)*np.multiply(grads[i],grads[i])\n",
    "                        self.thetas[i] = self.thetas[i] - self.eta*grads[i]/((np.sqrt(ut[i])+delta))\n",
    "                elif(self.optimizer == 'adam'):\n",
    "                    activation,preactivation = self.feed_forward(X_train[x:x+self.batchsize],self.thetas,self.no_of_layers)\n",
    "                    grads = self.back_propagate(activation,preactivation,self.thetas,Y_train[x:x+self.batchsize])\n",
    "                    for i in self.thetas.keys():\n",
    "                        ut[i] = beta1*ut[i] + (1-beta1)*grads[i]\n",
    "                        uthat = ut[i]/(1 - pow(beta1,t+1))\n",
    "                        vt[i] = beta2*vt[i] + (1-beta2)*np.multiply(grads[i],grads[i])\n",
    "                        vthat = vt[i]/(1 - pow(beta2,t+1))\n",
    "                        self.thetas[i] = self.thetas[i] - self.eta*uthat/((np.sqrt(vthat) + delta))\n",
    "                elif(self.optimizer == 'nadam'):\n",
    "                    activation,preactivation = self.feed_forward(X_train[x:x+self.batchsize],self.thetas,self.no_of_layers)\n",
    "                    grads = self.back_propagate(activation,preactivation,self.thetas,Y_train[x:x+self.batchsize])\n",
    "                    for i in self.thetas.keys():\n",
    "                        ut[i] = beta1*ut[i] + (1-beta1)*grads[i]\n",
    "                        uthat = ut[i]/(1 - pow(beta1,t+1))\n",
    "                        vt[i] = beta2*vt[i] + (1-beta2)*np.multiply(grads[i],grads[i])\n",
    "                        vthat = vt[i]/(1 - pow(beta2,t+1))\n",
    "                        self.thetas[i] = self.thetas[i] - (self.eta*(beta1*uthat + (1-beta1)*grads[i]/(1-pow(beta1,t+1))))/(np.sqrt(vthat) + delta)  \n",
    "                step+=1\n",
    "            yhat = self.predict(X_train)\n",
    "            accuracy = self.accuracy_score(Y_train,yhat)\n",
    "            print(accuracy)\n",
    "            loss = self.calculateLoss(yhat,Y_train)\n",
    "            print(loss)\n",
    "            self.loss_list.append(loss)\n",
    "            print(self.thetas['W1'][0][0])\n",
    "                \n",
    "    def predict(self,X):\n",
    "            activation,preactivation = self.feed_forward(X[:],self.thetas,self.no_of_layers)\n",
    "            return activation[-1]\n",
    "\n",
    "    def accuracy_score(self,Y,yhat):\n",
    "        correct = 0\n",
    "        for x in range(len(yhat)):\n",
    "            if(np.argmax(yhat[x]) == Y[x]):\n",
    "                correct+=1\n",
    "        return (correct/len(Y)*100)\n",
    "    def calculateLoss(self,yHat,yBatch):\n",
    "        loss=0\n",
    "        if(self.loss == 'cross'):\n",
    "            for x in range(len(yHat)):\n",
    "                loss += (-1)*np.log(yHat[x][yBatch[x]] + 1e-9)\n",
    "            return loss/len(yHat)\n",
    "        if(self.loss == 'mse'):\n",
    "                error = (yHat-yBatch)\n",
    "                error=error**2\n",
    "                loss = np.sum(error,axis=0) \n",
    "                loss = np.sum(error)  \n",
    "                loss = loss/2\n",
    "        return loss\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = NeuralNetwork('nadam',100,784,10,4,[128,128],10,0.002,'he','relu','cross')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.75925925925925\n",
      "[0.39840492]\n",
      "0.10355508371943782\n",
      "87.71666666666667\n",
      "[0.34583069]\n",
      "0.16469104870502735\n",
      "89.68333333333334\n",
      "[0.28195652]\n",
      "0.16990542427303887\n",
      "91.30185185185185\n",
      "[0.23659526]\n",
      "0.17427985836232412\n",
      "91.7925925925926\n",
      "[0.22100974]\n",
      "0.17753088697690123\n",
      "92.08333333333333\n",
      "[0.21466353]\n",
      "0.15849957604890996\n",
      "92.4462962962963\n",
      "[0.20203071]\n",
      "0.16656274645780664\n",
      "93.15\n",
      "[0.18199761]\n",
      "0.16262139924838268\n",
      "93.44259259259259\n",
      "[0.17448298]\n",
      "0.16689358493952625\n",
      "93.89999999999999\n",
      "[0.16108577]\n",
      "0.17711035031053976\n"
     ]
    }
   ],
   "source": [
    "NN.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.thetas = NN.initialization_list[NN.initialization]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs5\n"
     ]
    }
   ],
   "source": [
    "beta1 = 0.9\n",
    "beta2 = 0.99\n",
    "ut = {}\n",
    "vt = {}\n",
    "delta = 1e-9\n",
    "print('epochs' + str(t))\n",
    "for x in NN.thetas.keys():\n",
    "    ut[x] = 0\n",
    "    vt[x] = 0\n",
    "for x in range(0,X_train.shape[0],NN.batchsize):\n",
    "    act,pre = NN.feed_forward(X_train[x:x+NN.batchsize],NN.thetas,NN.no_of_layers)\n",
    "    grads = NN.back_propagate(act,pre,NN.thetas,Y_train[x:x+NN.batchsize])\n",
    "    for i in NN.thetas.keys():\n",
    "        ut[i] = beta1*ut[i] + (1-beta1)*grads[i]\n",
    "        uthat = ut[i]/(1 - pow(beta1,t+1))\n",
    "        vt[i] = beta2*vt[i] + (1-beta2)*np.multiply(grads[i],grads[i])\n",
    "        vthat = vt[i]/(1 - pow(beta2,t+1))\n",
    "        NN.thetas[i] = NN.thetas[i] - (NN.eta*(beta1*uthat + (1-beta1)*grads[i]/(1-pow(beta1,t+1))))/(np.sqrt(vthat) + delta) \n",
    "t += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15849957604890996"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.thetas['W1'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "act,pre = NN.feed_forward(X_train[:],NN.thetas,NN.no_of_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.08333333333333"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.accuracy_score(Y_train,act[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = NN.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93.89999999999999"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.accuracy_score(Y_train,yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
