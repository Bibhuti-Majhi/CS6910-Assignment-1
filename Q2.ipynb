{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Feed Forward Neural Network for Fashion_mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist, fashion_mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import wandb\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and Preprocessing of fashion_mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X,Y),(X_test,Y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping the X to flatten it and then dividing the pixels value by 255 to normalize them.\n",
    "X = X.reshape(len(X),784,1)\n",
    "X[0].shape\n",
    "X_test = X_test.reshape(len(X_test),784,1)\n",
    "X = X/255.0\n",
    "X_test = X_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,Y_train,Y_val = train_test_split(X,Y,test_size=0.1,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Made a class NeuralNetwork that contains all the necessary functions and parameters for a neural network.\n",
    "# Wrote code for Feedforward that is flexible enough to handle large inputs with variable layer sizes because of\n",
    "# the way I have handled the parameters initialization. Given the dimensions of each layer of the network allows the code to \n",
    "# make the necessary initializations and can be adjusted easily to your like.\n",
    "class NeuralNetwork:\n",
    "    def __init__(self,optimizer='nadam',batchsize=32,no_of_features = 784,no_of_classes = 10,no_of_layers=5,no_of_neurons_in_each_layer = [128,128,128],max_epochs = 10,eta = 0.001,initialization_method = 'he',activation_method = 'relu',loss = 'cross',weight_decay = 0.001):\n",
    "        self.optimizer  = optimizer\n",
    "        self.batchsize = batchsize\n",
    "        self.no_of_features = no_of_features\n",
    "        self.no_of_classes = no_of_classes\n",
    "        self.no_of_layers = no_of_layers\n",
    "        self.no_of_neurons_in_each_layer = no_of_neurons_in_each_layer\n",
    "        self.max_epochs = max_epochs\n",
    "        self.eta = eta\n",
    "        self.initialization = initialization_method\n",
    "        self.initialization_list = {'xavier':self.xavier_initialization,'uniform':self.uniform_initialization,'normal':self.normal_initialization,'he':self.he_initialization}\n",
    "        self.activation = activation_method\n",
    "        self.activation_list = {'sigmoid':self.sigmoid,'relu':self.Relu,'tanh':self.tanh}\n",
    "        self.activation_derivative = {'sigmoid':self.sigmoid_derivative,'relu':self.Relu_derivative,'tanh':self.tanh_derivative}\n",
    "        self.loss = loss\n",
    "        self.thetas = {}\n",
    "        self.loss_list = []\n",
    "        self.weight_decay = weight_decay\n",
    "    def one_hot(self,l,no_of_classes):\n",
    "        temp = np.array([0]*no_of_classes)\n",
    "        temp[l] = 1\n",
    "        return temp\n",
    "    def sigmoid(self,x):\n",
    "        return 1. / (1.+np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self,x):\n",
    "        return self.sigmoid(x) * (np.ones_like(x)-self.sigmoid(x))\n",
    "\n",
    "    def Relu(self,x):\n",
    "        for i in range(len(x)):\n",
    "            x[i] = x[i] / max(x[i])\n",
    "        return np.maximum(0,x)\n",
    "\n",
    "    def Relu_derivative(self,x):\n",
    "        # for i in range(len(x)):\n",
    "        #     x[i] = x[i] / max(x[i])\n",
    "        return 1*(x>0) \n",
    "\n",
    "    def tanh(self,x):\n",
    "        # for i in range(len(x)):\n",
    "        #     x[i] = x[i] / max(x[i])\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def tanh_derivative(self,x):\n",
    "        # for i in range(len(x)):\n",
    "        #     x[i] = x[i] / max(x[i])\n",
    "        return (1 - (np.tanh(x)**2))\n",
    "\n",
    "    def softmax(self,x):\n",
    "        # for i in range(len(x)):\n",
    "        #     x[i] = x[i] / max(x[i])\n",
    "        l = []\n",
    "        for i in range(len(x)):\n",
    "            l.append(np.exp(x[i])/np.sum(np.exp(x[i]),axis=0))\n",
    "        return np.array(l)\n",
    "        \n",
    "    def softmax_derivative(self,x):\n",
    "        # for i in range(len(x)):\n",
    "        #     x[i] = x[i] / max(x)\n",
    "        return self.softmax(x.reshape(1,x.shape[0],x.shape[1])) * (1-self.softmax(x.reshape(1,x.shape[0],x.shape[1])))\n",
    "\n",
    "\n",
    "\n",
    "    def he_initialization(self):\n",
    "        np.random.seed(42)\n",
    "        thetas = {}\n",
    "        for layer in range(1,self.no_of_layers):\n",
    "            if(layer == 1):\n",
    "                thetas['W'+str(layer)] = np.random.normal(0,1,size = (self.no_of_neurons_in_each_layer[layer-1],self.no_of_features)) * np.sqrt(2/(self.no_of_neurons_in_each_layer[layer-1]))\n",
    "                thetas['b'+str(layer)] = np.random.normal(0,1,size = (self.no_of_neurons_in_each_layer[layer-1],1))*np.sqrt(2/self.no_of_neurons_in_each_layer[layer-1])\n",
    "            elif(layer == self.no_of_layers-1):\n",
    "                thetas['W'+str(layer)] = np.random.normal(0,1,size = (self.no_of_classes,self.no_of_neurons_in_each_layer[layer-2])) * np.sqrt(2/(self.no_of_classes))\n",
    "                thetas['b'+str(layer)] = np.random.normal(0,1,size = (self.no_of_classes,1)) * np.sqrt(2/(self.no_of_classes))\n",
    "            else:\n",
    "                thetas['W'+str(layer)] = np.random.normal(0,1,size = (self.no_of_neurons_in_each_layer[layer-1],self.no_of_neurons_in_each_layer[layer-2])) * np.sqrt(2/(self.no_of_neurons_in_each_layer[layer-1]))\n",
    "                thetas['b'+str(layer)] = np.random.normal(0,1,size = (self.no_of_neurons_in_each_layer[layer-1],1)) * np.sqrt(2/self.no_of_neurons_in_each_layer[layer-1])\n",
    "        return thetas\n",
    "\n",
    "    def xavier_initialization(self):\n",
    "        np.random.seed(42)\n",
    "        thetas = {}\n",
    "        for layer in range(1,self.no_of_layers):\n",
    "            if(layer == 1):\n",
    "                thetas['W'+str(layer)] = np.random.randn(self.no_of_neurons_in_each_layer[layer-1],self.no_of_features) * np.sqrt(2/(self.no_of_neurons_in_each_layer[layer-1]+self.no_of_features))\n",
    "                thetas['b'+str(layer)] = np.zeros((self.no_of_neurons_in_each_layer[layer-1],1))\n",
    "            elif(layer == self.no_of_layers-1):\n",
    "                thetas['W'+str(layer)] = np.random.randn(self.no_of_classes,self.no_of_neurons_in_each_layer[layer-2]) * np.sqrt(2/(self.no_of_classes + self.no_of_neurons_in_each_layer[layer-2]))\n",
    "                thetas['b'+str(layer)] = np.zeros((self.no_of_classes,1))\n",
    "            else:\n",
    "                # print(layer)\n",
    "                # print(self.no_of_neurons_in_each_layer[layer-1])\n",
    "                # print(self.no_of_neurons_in_each_layer[layer-2])\n",
    "                thetas['W'+str(layer)] = np.random.randn(self.no_of_neurons_in_each_layer[layer-1],self.no_of_neurons_in_each_layer[layer-2]) * np.sqrt(2/(self.no_of_neurons_in_each_layer[layer-1]+self.no_of_neurons_in_each_layer[layer-2]))\n",
    "                thetas['b'+str(layer)] = np.zeros((self.no_of_neurons_in_each_layer[layer-1],1))\n",
    "        return thetas\n",
    "\n",
    "    def uniform_initialization(self):\n",
    "        thetas = {}\n",
    "        np.random.seed(42)\n",
    "        for layer in range(1,self.no_of_layers):\n",
    "            if(layer == 1):\n",
    "                thetas['W'+str(layer)] = np.random.default_rng().uniform(low = -0.7,high =0.7,size = (self.no_of_neurons_in_each_layer[layer-1],self.no_of_features)) #* np.sqrt(2/(no_of_neurons_in_each_layer[layer-1]+no_of_features))\n",
    "                thetas['b'+str(layer)] = np.random.default_rng().uniform(low = -0.7,high =0.7,size = (self.no_of_neurons_in_each_layer[layer-1],1))\n",
    "            elif(layer == self.no_of_layers-1):\n",
    "                thetas['W'+str(layer)] = np.random.default_rng().uniform(low = -0.7,high =0.7,size = (self.no_of_classes,self.no_of_neurons_in_each_layer[layer-2])) #* np.sqrt(2/(no_of_classes + no_of_neurons_in_each_layer[layer-2]))\n",
    "                thetas['b'+str(layer)] = np.random.default_rng().uniform(low = -0.7,high =0.7,size = (self.no_of_classes,1))\n",
    "            else:\n",
    "                thetas['W'+str(layer)] = np.random.default_rng().uniform(low = -0.7,high =0.7,size =(self.no_of_neurons_in_each_layer[layer-1],self.no_of_neurons_in_each_layer[layer-2])) #*  np.sqrt(2/(no_of_neurons_in_each_layer[layer-1]+no_of_neurons_in_each_layer[layer-2]))\n",
    "                thetas['b'+str(layer)] = np.random.default_rng().uniform(low = -0.7,high =0.7,size = (self.no_of_neurons_in_each_layer[layer-1],1))\n",
    "        return thetas\n",
    "\n",
    "    def normal_initialization(self):\n",
    "        thetas = {}\n",
    "        np.random.seed(42)\n",
    "        for layer in range(1,self.no_of_layers):\n",
    "            if(layer == 1):\n",
    "                thetas['W'+str(layer)] = np.random.uniform(low = -0.7,high =0.7,size = (self.no_of_neurons_in_each_layer[layer-1],self.no_of_features)) #* np.sqrt(2/(no_of_neurons_in_each_layer[layer-1]+no_of_features))\n",
    "                thetas['b'+str(layer)] = np.zeros((self.no_of_neurons_in_each_layer[layer-1],1))\n",
    "            elif(layer == self.no_of_layers-1):\n",
    "                thetas['W'+str(layer)] = np.random.uniform(low = -0.7,high =0.7,size = (self.no_of_classes,self.no_of_neurons_in_each_layer[layer-2])) #* np.sqrt(2/(no_of_classes + no_of_neurons_in_each_layer[layer-2]))\n",
    "                thetas['b'+str(layer)] = np.zeros((self.no_of_classes,1))\n",
    "            else:\n",
    "                thetas['W'+str(layer)] = np.random.uniform(low = -0.7,high =0.7,size =(self.no_of_neurons_in_each_layer[layer-1],self.no_of_neurons_in_each_layer[layer-2])) #*  np.sqrt(2/(no_of_neurons_in_each_layer[layer-1]+no_of_neurons_in_each_layer[layer-2]))\n",
    "                thetas['b'+str(layer)] = np.zeros((self.no_of_neurons_in_each_layer[layer-1],1))\n",
    "        return thetas\n",
    "\n",
    "    def feed_forward(self,data,thetas,layers):\n",
    "        pre_activation = [1]*(layers)\n",
    "        activation  = [1]*(layers)\n",
    "        activation[0] = data\n",
    "        for layer_no in range(1,layers):\n",
    "            W = 'W' + str(layer_no)\n",
    "            b = 'b' + str(layer_no)\n",
    "            pre_activation[layer_no] = np.add(np.matmul(thetas[W],activation[layer_no - 1]),thetas[b])\n",
    "            if(layer_no == layers-1):\n",
    "                activation[layer_no] = self.softmax(pre_activation[layer_no])\n",
    "            else:\n",
    "                activation[layer_no] = self.activation_list[self.activation](pre_activation[layer_no])\n",
    "        return activation,pre_activation\n",
    "\n",
    "    def back_propagate(self,h,a,thetas,Y):\n",
    "        grads = {}\n",
    "        for x in thetas.keys():\n",
    "            grads[x] = 0\n",
    "        for x in range(len(Y)):\n",
    "            temp = h[-1][x] - self.one_hot(Y[x],self.no_of_classes).reshape(self.no_of_classes,1)\n",
    "            if(self.loss == 'mse'):\n",
    "                temp = (temp*self.softmax_derivative(a[-1][x])).reshape(self.no_of_classes,1)\n",
    "            for k in range(self.no_of_layers-1,0,-1):\n",
    "                W = 'W' + str(k)\n",
    "                b = 'b' + str(k)\n",
    "                grads[W] += np.matmul(temp,h[k-1][x].T)/self.batchsize\n",
    "                grads[b] += temp/self.batchsize\n",
    "                if(k == 1):\n",
    "                    break\n",
    "                temp = np.matmul(thetas[W].T,temp)\n",
    "                temp = np.multiply(temp,self.activation_derivative[self.activation](a[k-1][x]))\n",
    "        return grads\n",
    "\n",
    "    def momentumUpdate(self,t,maxm=.999):\n",
    "        x=np.log(np.floor(t/250)+1)/np.log(2)\n",
    "        x=1-2**(-1-x)\n",
    "        return min(x,maxm)\n",
    "\n",
    "    def getGamma(self,epoch):\n",
    "        x=np.log((epoch/250)+1)\n",
    "        x=-1-1*(x)\n",
    "        x=2**x\n",
    "        x=1-x\n",
    "        return min(x,.9)\n",
    "\n",
    "    def fit(self,X_train,Y_train):\n",
    "        self.thetas = self.initialization_list[self.initialization]()\n",
    "        delta = 1e-9\n",
    "        grads = {}\n",
    "        for i in self.thetas.keys():\n",
    "            grads[i] = 0\n",
    "        for t in range(self.max_epochs):\n",
    "            #previous_update\n",
    "            ut = {}\n",
    "            vt = {}\n",
    "            gamma = self.getGamma(t+1)\n",
    "            beta = self.momentumUpdate(t+1)\n",
    "            for i in self.thetas.keys():\n",
    "                ut[i] = 0\n",
    "                vt[i] = 0\n",
    "            params_look_ahead = {}\n",
    "            step = 1\n",
    "            for x in range(0,X_train.shape[0],self.batchsize):\n",
    "                beta1 = 0.9#self.momentumUpdate(step)\n",
    "                beta2 = 0.99#self.momentumUpdate(step)\n",
    "                if(self.optimizer == 'nesterov'):\n",
    "                    for i in self.thetas.keys():\n",
    "                        params_look_ahead[i] = self.thetas[i] - beta1*ut[i]\n",
    "                    activation,preactivation = self.feed_forward(X_train[x:x+self.batchsize],self.thetas,self.no_of_layers)\n",
    "                    grads = self.back_propagate(activation,preactivation,params_look_ahead,Y_train[x:x+self.batchsize])\n",
    "                    for i in self.thetas.keys():\n",
    "                        ut[i] = beta1*ut[i] + (1-beta1)*grads[i]\n",
    "                        self.thetas[i] = self.thetas[i] - self.eta*ut[i] - self.eta*self.weight_decay*self.thetas[i]\n",
    "                elif(self.optimizer == 'mgd'):\n",
    "                    activation,preactivation = self.feed_forward(X_train[x:x+self.batchsize],self.thetas,self.no_of_layers)\n",
    "                    grads = self.back_propagate(activation,preactivation,self.thetas,Y_train[x:x+self.batchsize])     \n",
    "                    for i in self.thetas.keys():\n",
    "                        ut[i] = gamma*ut[i] + grads[i]\n",
    "                        self.thetas[i] = self.thetas[i] - self.eta*ut[i] - self.eta*self.weight_decay*self.thetas[i]\n",
    "                elif(self.optimizer == 'sgd'):\n",
    "                    activation,preactivation = self.feed_forward(X_train[x:x+self.batchsize],self.thetas,self.no_of_layers)\n",
    "                    grads = self.back_propagate(activation,preactivation,self.thetas,Y_train[x:x+self.batchsize])\n",
    "                    for i in self.thetas.keys():\n",
    "                        self.thetas[i] = self.thetas[i] - self.eta*grads[i] - self.eta*self.weight_decay*self.thetas[i]\n",
    "                elif(self.optimizer == 'RMSprop'):\n",
    "                    activation,preactivation = self.feed_forward(X_train[x:x+self.batchsize],self.thetas,self.no_of_layers)\n",
    "                    grads = self.back_propagate(activation,preactivation,self.thetas,Y_train[x:x+self.batchsize])\n",
    "                    for i in self.thetas.keys():\n",
    "                        ut[i] = beta*ut[i] + (1-beta)*np.multiply(grads[i],grads[i])\n",
    "                        self.thetas[i] = self.thetas[i] - self.eta*grads[i]/((np.sqrt(ut[i])+delta)) - self.eta*self.weight_decay*self.thetas[i]\n",
    "                elif(self.optimizer == 'adam'):\n",
    "                    activation,preactivation = self.feed_forward(X_train[x:x+self.batchsize],self.thetas,self.no_of_layers)\n",
    "                    grads = self.back_propagate(activation,preactivation,self.thetas,Y_train[x:x+self.batchsize])\n",
    "                    for i in self.thetas.keys():\n",
    "                        ut[i] = beta1*ut[i] + (1-beta1)*grads[i]\n",
    "                        uthat = ut[i]/(1 - pow(beta1,t+1))\n",
    "                        vt[i] = beta2*vt[i] + (1-beta2)*np.multiply(grads[i],grads[i])\n",
    "                        vthat = vt[i]/(1 - pow(beta2,t+1))\n",
    "                        self.thetas[i] = self.thetas[i] - self.eta*uthat/((np.sqrt(vthat) + delta)) - self.eta*self.weight_decay*self.thetas[i]\n",
    "                elif(self.optimizer == 'nadam'):\n",
    "                    activation,preactivation = self.feed_forward(X_train[x:x+self.batchsize],self.thetas,self.no_of_layers)\n",
    "                    grads = self.back_propagate(activation,preactivation,self.thetas,Y_train[x:x+self.batchsize])\n",
    "                    for i in self.thetas.keys():\n",
    "                        ut[i] = beta1*ut[i] + (1-beta1)*grads[i]\n",
    "                        uthat = ut[i]/(1 - pow(beta1,t+1))\n",
    "                        vt[i] = beta2*vt[i] + (1-beta2)*np.multiply(grads[i],grads[i])\n",
    "                        vthat = vt[i]/(1 - pow(beta2,t+1))\n",
    "                        self.thetas[i] = self.thetas[i] - (self.eta*(beta1*uthat + (1-beta1)*grads[i]/(1-pow(beta1,t+1))))/(np.sqrt(vthat) + delta) - self.eta*self.weight_decay*self.thetas[i]\n",
    "                step+=1\n",
    "            yhat = self.predict(X_train)\n",
    "            training_accuracy = self.accuracy_score(Y_train,yhat)\n",
    "            # print(training_accuracy)\n",
    "            training_loss = self.calculateLoss(yhat,Y_train)\n",
    "            # print(training_loss)\n",
    "            self.loss_list.append(training_loss)\n",
    "            yhat = self.predict(X_val)\n",
    "            validation_accuracy = self.accuracy_score(Y_val,yhat)\n",
    "            # print(validation_accuracy)\n",
    "            validation_loss = self.calculateLoss(yhat,Y_val)\n",
    "            # print(validation_loss)\n",
    "            #wandb.log({'training_accuracy' : training_accuracy, 'validation_accuracy' : validation_accuracy,'training_loss' : training_loss, 'validation_loss' : validation_loss,'epoch':t+1})\n",
    "                \n",
    "    def predict(self,X):\n",
    "            activation,preactivation = self.feed_forward(X[:],self.thetas,self.no_of_layers)\n",
    "            return activation[-1]\n",
    "\n",
    "    def accuracy_score(self,Y,yhat):\n",
    "        correct = 0\n",
    "        for x in range(len(yhat)):\n",
    "            if(np.argmax(yhat[x]) == Y[x]):\n",
    "                correct+=1\n",
    "        return (correct/len(Y)*100)\n",
    "    def calculateLoss(self,yHat,yBatch):\n",
    "        loss=0\n",
    "        l2=0\n",
    "        if(self.loss == 'cross'):\n",
    "            for x in range(len(yHat)):\n",
    "                loss += (-1)*np.log(yHat[x][yBatch[x]] + 1e-9)\n",
    "            for x in self.thetas.keys():\n",
    "                l2 += np.linalg.norm(self.thetas[x])\n",
    "            l2 = (self.weight_decay*l2)/2\n",
    "            return (loss + l2)/len(yHat)\n",
    "        if(self.loss == 'mse'):\n",
    "                # error = (yHat-yBatch)\n",
    "                # error=error**2\n",
    "                # loss = np.sum(error,axis=0) \n",
    "                # loss = np.sum(error)  \n",
    "                # loss = loss/2\n",
    "            for x in range(len(yHat)):\n",
    "                loss += np.sum((self.one_hot(yBatch[x],self.no_of_classes).reshape(NN.no_of_classes,1) - yHat[x])**2,axis = 0)\n",
    "            for x in self.thetas.keys():\n",
    "                l2 += np.linalg.norm(self.thetas[x])\n",
    "            l2 = (self.weight_decay*l2)/2\n",
    "            return (loss[0] + l2)/len(yHat)\n",
    "        return loss\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of usage of feed Forward.\n",
    "optimizer = 'sgd'\n",
    "batchsize = 16\n",
    "no_of_features = 784\n",
    "no_of_classes = 10\n",
    "no_of_layers = 5\n",
    "no_of_neurons_in_each_layer = [128,128,128]\n",
    "max_epochs = 10\n",
    "eta = 0.1\n",
    "initialization_method = 'uniform'\n",
    "activation_method = 'sigmoid'\n",
    "weight_decay = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = NeuralNetwork(optimizer,batchsize,no_of_features,no_of_classes,no_of_layers,no_of_neurons_in_each_layer,max_epochs,eta,initialization_method,activation_method,weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = NN.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(NN.accuracy_score(Y_train,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
